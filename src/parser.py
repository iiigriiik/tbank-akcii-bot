# src/parser.py
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–µ—Ä –∞–∫—Ü–∏–π –¢-–ë–∞–Ω–∫–∞

import requests
from bs4 import BeautifulSoup
import html
import re
import os
from datetime import datetime
import time

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
# –¢–æ–∫–µ–Ω –∏ ID –∫–∞–Ω–∞–ª–∞ –±—É–¥—É—Ç –±—Ä–∞—Ç—å—Å—è –∏–∑ config.py
import os
BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

BLOG_URL = "https://www.tbank.ru/finance/blog/"
DATA_FILE = "../data/sent_promos.txt"  # –ù–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ, –≤ –ø–∞–ø–∫–µ data

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# === –§–£–ù–ö–¶–ò–ò ===
def load_sent_urls():
    if not os.path.exists(DATA_FILE):
        return set()
    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f if line.strip())
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}")
        return set()

def save_sent_url(url):
    try:
        with open(DATA_FILE, "a", encoding="utf-8") as f:
            f.write(url + "\n")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏: {e}")

def get_article_excerpt(url):
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return "–¢–µ–∫—Å—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"

        soup = BeautifulSoup(response.text, "html.parser")
        content = soup.find("div", class_="Article__content")
        if not content:
            content = soup.find("div", {"data-pb-type": "content"})

        if content:
            paragraphs = content.find_all("p", limit=3)
            text = " ".join(p.get_text(strip=True) for p in paragraphs)
            text = re.sub(r'\s+', ' ', text).strip()
            return (text[:280] + "...") if len(text) > 280 else text
        else:
            return "–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –ø–æ —Å—Å—ã–ª–∫–µ"
    except Exception as e:
        return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–µ–∫—Å—Ç–∞"

def send_to_telegram(title, url, description=""):
    safe_title = html.escape(title)
    safe_desc = html.escape(description)

    message = f"""
üöÄ <b>–ù–æ–≤–∞—è –∞–∫—Ü–∏—è –æ—Ç –¢‚Äë–ë–∞–Ω–∫–∞!</b>

üìå <b>{safe_title}</b>

{safe_desc}

üîó <a href="{url}">–ü–æ–¥—Ä–æ–±–Ω–µ–µ –Ω–∞ —Å–∞–π—Ç–µ</a>
    """.strip()

    api_url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    data = {
        "chat_id": CHANNEL_ID,
        "text": message,
        "parse_mode": "HTML",
        "disable_web_page_preview": False
    }

    try:
        response = requests.post(api_url, data=data, timeout=10)
        if response.status_code == 200:
            return True, None
        else:
            return False, f"HTTP {response.status_code}: {response.text}"
    except Exception as e:
        return False, str(e)

# === –ó–ê–ü–£–°–ö ===
print(f"[{datetime.now().strftime('%H:%M:%S')}] –ó–∞–ø—É—Å–∫ parser.py...")

sent_urls = load_sent_urls()
new_count = 0

try:
    response = requests.get(BLOG_URL, headers=headers, timeout=15)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    links = soup.find_all("a", href=True)
    new_promos = []

    for link in links:
        href = link["href"]
        if href.startswith("/finance/blog/") and len(href) > 20:
            full_url = "https://www.tbank.ru" + href
            if full_url not in sent_urls:
                title_elem = link.find("h3") or link.find(class_=re.compile("Typography", re.I))
                title = title_elem.get_text(strip=True) if title_elem else "–ù–æ–≤–∞—è –∞–∫—Ü–∏—è"
                if len(title) > 100:
                    title = title[:100] + "..."
                new_promos.append((title, full_url))

    new_promos = list(set(new_promos))
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö: {len(new_promos)}")

    for title, url in new_promos:
        print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞—é: {title}")
        desc = get_article_excerpt(url)
        success, error = send_to_telegram(title, url, desc)
        if success:
            print(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {title}")
            save_sent_url(url)
            new_count += 1
            time.sleep(1.5)
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {title} ‚Üí {error}")

except Exception as e:
    print(f"üí• –û—à–∏–±–∫–∞: {e}")

print(f"‚úÖ –ì–æ—Ç–æ–≤–æ. –ù–æ–≤—ã—Ö –∞–∫—Ü–∏–π: {new_count}")